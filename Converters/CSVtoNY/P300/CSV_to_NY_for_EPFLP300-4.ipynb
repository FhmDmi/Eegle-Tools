{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MOABB to NY format\n",
    "\n",
    ".CSV to NY\n",
    "\n",
    "This code convert the databases from .CSV format to NY format which it can be easily read in Julia and Python. \n",
    "\n",
    "The .npz file holds the data and the stimulation vector while the .yml file holds the metadata. \n",
    "\n",
    "It has been specifically conceived for BCI data.\n",
    "\n",
    "This script is for EPFLP300-4 (Door - run 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import essential libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob, os, sys, yaml\n",
    "from yaml import CLoader as Loader, CDumper as Dumper\n",
    "import moabb\n",
    "from moabb.paradigms import P300\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\doumif\\.conda\\envs\\py310\\lib\\site-packages\\moabb\\datasets\\preprocessing.py:88: RuntimeWarning: Limited 1 annotation(s) that were expanding outside the data range.\n",
      "  raw.set_annotations(annotations)\n",
      "c:\\Users\\doumif\\.conda\\envs\\py310\\lib\\site-packages\\moabb\\datasets\\preprocessing.py:88: RuntimeWarning: Limited 3 annotation(s) that were expanding outside the data range.\n",
      "  raw.set_annotations(annotations)\n",
      "c:\\Users\\doumif\\.conda\\envs\\py310\\lib\\site-packages\\moabb\\datasets\\preprocessing.py:88: RuntimeWarning: Limited 1 annotation(s) that were expanding outside the data range.\n",
      "  raw.set_annotations(annotations)\n",
      "c:\\Users\\doumif\\.conda\\envs\\py310\\lib\\site-packages\\moabb\\datasets\\preprocessing.py:88: RuntimeWarning: Limited 1 annotation(s) that were expanding outside the data range.\n",
      "  raw.set_annotations(annotations)\n",
      "c:\\Users\\doumif\\.conda\\envs\\py310\\lib\\site-packages\\moabb\\datasets\\preprocessing.py:88: RuntimeWarning: Limited 1 annotation(s) that were expanding outside the data range.\n",
      "  raw.set_annotations(annotations)\n",
      "c:\\Users\\doumif\\.conda\\envs\\py310\\lib\\site-packages\\moabb\\datasets\\preprocessing.py:88: RuntimeWarning: Limited 1 annotation(s) that were expanding outside the data range.\n",
      "  raw.set_annotations(annotations)\n",
      "c:\\Users\\doumif\\.conda\\envs\\py310\\lib\\site-packages\\moabb\\datasets\\preprocessing.py:88: RuntimeWarning: Limited 2 annotation(s) that were expanding outside the data range.\n",
      "  raw.set_annotations(annotations)\n",
      "c:\\Users\\doumif\\.conda\\envs\\py310\\lib\\site-packages\\moabb\\datasets\\preprocessing.py:88: RuntimeWarning: Limited 2 annotation(s) that were expanding outside the data range.\n",
      "  raw.set_annotations(annotations)\n",
      "c:\\Users\\doumif\\.conda\\envs\\py310\\lib\\site-packages\\moabb\\datasets\\preprocessing.py:88: RuntimeWarning: Limited 3 annotation(s) that were expanding outside the data range.\n",
      "  raw.set_annotations(annotations)\n",
      "c:\\Users\\doumif\\.conda\\envs\\py310\\lib\\site-packages\\moabb\\datasets\\preprocessing.py:88: RuntimeWarning: Limited 2 annotation(s) that were expanding outside the data range.\n",
      "  raw.set_annotations(annotations)\n",
      "c:\\Users\\doumif\\.conda\\envs\\py310\\lib\\site-packages\\moabb\\datasets\\preprocessing.py:88: RuntimeWarning: Limited 1 annotation(s) that were expanding outside the data range.\n",
      "  raw.set_annotations(annotations)\n",
      "c:\\Users\\doumif\\.conda\\envs\\py310\\lib\\site-packages\\moabb\\datasets\\preprocessing.py:88: RuntimeWarning: Limited 2 annotation(s) that were expanding outside the data range.\n",
      "  raw.set_annotations(annotations)\n",
      "c:\\Users\\doumif\\.conda\\envs\\py310\\lib\\site-packages\\moabb\\datasets\\preprocessing.py:88: RuntimeWarning: Limited 1 annotation(s) that were expanding outside the data range.\n",
      "  raw.set_annotations(annotations)\n",
      "c:\\Users\\doumif\\.conda\\envs\\py310\\lib\\site-packages\\moabb\\datasets\\preprocessing.py:88: RuntimeWarning: Limited 1 annotation(s) that were expanding outside the data range.\n",
      "  raw.set_annotations(annotations)\n",
      "c:\\Users\\doumif\\.conda\\envs\\py310\\lib\\site-packages\\moabb\\datasets\\preprocessing.py:88: RuntimeWarning: Limited 2 annotation(s) that were expanding outside the data range.\n",
      "  raw.set_annotations(annotations)\n",
      "c:\\Users\\doumif\\.conda\\envs\\py310\\lib\\site-packages\\moabb\\datasets\\preprocessing.py:88: RuntimeWarning: Limited 2 annotation(s) that were expanding outside the data range.\n",
      "  raw.set_annotations(annotations)\n",
      "c:\\Users\\doumif\\.conda\\envs\\py310\\lib\\site-packages\\moabb\\datasets\\preprocessing.py:88: RuntimeWarning: Limited 1 annotation(s) that were expanding outside the data range.\n",
      "  raw.set_annotations(annotations)\n",
      "c:\\Users\\doumif\\.conda\\envs\\py310\\lib\\site-packages\\moabb\\datasets\\preprocessing.py:88: RuntimeWarning: Limited 2 annotation(s) that were expanding outside the data range.\n",
      "  raw.set_annotations(annotations)\n",
      "c:\\Users\\doumif\\.conda\\envs\\py310\\lib\\site-packages\\moabb\\datasets\\preprocessing.py:88: RuntimeWarning: Limited 2 annotation(s) that were expanding outside the data range.\n",
      "  raw.set_annotations(annotations)\n",
      "c:\\Users\\doumif\\.conda\\envs\\py310\\lib\\site-packages\\moabb\\datasets\\preprocessing.py:88: RuntimeWarning: Limited 2 annotation(s) that were expanding outside the data range.\n",
      "  raw.set_annotations(annotations)\n",
      "c:\\Users\\doumif\\.conda\\envs\\py310\\lib\\site-packages\\moabb\\datasets\\preprocessing.py:88: RuntimeWarning: Limited 2 annotation(s) that were expanding outside the data range.\n",
      "  raw.set_annotations(annotations)\n",
      "c:\\Users\\doumif\\.conda\\envs\\py310\\lib\\site-packages\\moabb\\datasets\\preprocessing.py:88: RuntimeWarning: Limited 1 annotation(s) that were expanding outside the data range.\n",
      "  raw.set_annotations(annotations)\n",
      "c:\\Users\\doumif\\.conda\\envs\\py310\\lib\\site-packages\\moabb\\datasets\\preprocessing.py:88: RuntimeWarning: Limited 1 annotation(s) that were expanding outside the data range.\n",
      "  raw.set_annotations(annotations)\n",
      "c:\\Users\\doumif\\.conda\\envs\\py310\\lib\\site-packages\\moabb\\datasets\\preprocessing.py:88: RuntimeWarning: Limited 1 annotation(s) that were expanding outside the data range.\n",
      "  raw.set_annotations(annotations)\n",
      "c:\\Users\\doumif\\.conda\\envs\\py310\\lib\\site-packages\\moabb\\datasets\\preprocessing.py:278: UserWarning: warnEpochs <Epochs | 137 events (all good), 0 – 1 s (baseline off), ~68.6 MiB, data loaded,\n",
      " 'Target': 23\n",
      " 'NonTarget': 114>\n",
      "  warn(f\"warnEpochs {epochs}\")\n",
      "c:\\Users\\doumif\\.conda\\envs\\py310\\lib\\site-packages\\moabb\\datasets\\preprocessing.py:278: UserWarning: warnEpochs <Epochs | 129 events (all good), 0 – 1 s (baseline off), ~64.6 MiB, data loaded,\n",
      " 'Target': 21\n",
      " 'NonTarget': 108>\n",
      "  warn(f\"warnEpochs {epochs}\")\n",
      "c:\\Users\\doumif\\.conda\\envs\\py310\\lib\\site-packages\\moabb\\datasets\\preprocessing.py:278: UserWarning: warnEpochs <Epochs | 137 events (all good), 0 – 1 s (baseline off), ~68.6 MiB, data loaded,\n",
      " 'Target': 23\n",
      " 'NonTarget': 114>\n",
      "  warn(f\"warnEpochs {epochs}\")\n",
      "c:\\Users\\doumif\\.conda\\envs\\py310\\lib\\site-packages\\moabb\\datasets\\preprocessing.py:278: UserWarning: warnEpochs <Epochs | 149 events (all good), 0 – 1 s (baseline off), ~74.6 MiB, data loaded,\n",
      " 'Target': 25\n",
      " 'NonTarget': 124>\n",
      "  warn(f\"warnEpochs {epochs}\")\n",
      "c:\\Users\\doumif\\.conda\\envs\\py310\\lib\\site-packages\\moabb\\datasets\\preprocessing.py:278: UserWarning: warnEpochs <Epochs | 149 events (all good), 0 – 1 s (baseline off), ~74.6 MiB, data loaded,\n",
      " 'Target': 25\n",
      " 'NonTarget': 124>\n",
      "  warn(f\"warnEpochs {epochs}\")\n",
      "c:\\Users\\doumif\\.conda\\envs\\py310\\lib\\site-packages\\moabb\\datasets\\preprocessing.py:278: UserWarning: warnEpochs <Epochs | 149 events (all good), 0 – 1 s (baseline off), ~74.6 MiB, data loaded,\n",
      " 'Target': 25\n",
      " 'NonTarget': 124>\n",
      "  warn(f\"warnEpochs {epochs}\")\n",
      "c:\\Users\\doumif\\.conda\\envs\\py310\\lib\\site-packages\\moabb\\datasets\\preprocessing.py:278: UserWarning: warnEpochs <Epochs | 124 events (all good), 0 – 1 s (baseline off), ~62.1 MiB, data loaded,\n",
      " 'Target': 20\n",
      " 'NonTarget': 104>\n",
      "  warn(f\"warnEpochs {epochs}\")\n",
      "c:\\Users\\doumif\\.conda\\envs\\py310\\lib\\site-packages\\moabb\\datasets\\preprocessing.py:278: UserWarning: warnEpochs <Epochs | 142 events (all good), 0 – 1 s (baseline off), ~71.1 MiB, data loaded,\n",
      " 'Target': 24\n",
      " 'NonTarget': 118>\n",
      "  warn(f\"warnEpochs {epochs}\")\n",
      "c:\\Users\\doumif\\.conda\\envs\\py310\\lib\\site-packages\\moabb\\datasets\\preprocessing.py:278: UserWarning: warnEpochs <Epochs | 129 events (all good), 0 – 1 s (baseline off), ~64.6 MiB, data loaded,\n",
      " 'Target': 22\n",
      " 'NonTarget': 107>\n",
      "  warn(f\"warnEpochs {epochs}\")\n",
      "c:\\Users\\doumif\\.conda\\envs\\py310\\lib\\site-packages\\moabb\\datasets\\preprocessing.py:278: UserWarning: warnEpochs <Epochs | 142 events (all good), 0 – 1 s (baseline off), ~71.1 MiB, data loaded,\n",
      " 'Target': 24\n",
      " 'NonTarget': 118>\n",
      "  warn(f\"warnEpochs {epochs}\")\n",
      "c:\\Users\\doumif\\.conda\\envs\\py310\\lib\\site-packages\\moabb\\datasets\\preprocessing.py:278: UserWarning: warnEpochs <Epochs | 137 events (all good), 0 – 1 s (baseline off), ~68.6 MiB, data loaded,\n",
      " 'Target': 23\n",
      " 'NonTarget': 114>\n",
      "  warn(f\"warnEpochs {epochs}\")\n",
      "c:\\Users\\doumif\\.conda\\envs\\py310\\lib\\site-packages\\moabb\\datasets\\preprocessing.py:278: UserWarning: warnEpochs <Epochs | 142 events (all good), 0 – 1 s (baseline off), ~71.1 MiB, data loaded,\n",
      " 'Target': 24\n",
      " 'NonTarget': 118>\n",
      "  warn(f\"warnEpochs {epochs}\")\n",
      "c:\\Users\\doumif\\.conda\\envs\\py310\\lib\\site-packages\\moabb\\datasets\\preprocessing.py:278: UserWarning: warnEpochs <Epochs | 137 events (all good), 0 – 1 s (baseline off), ~68.6 MiB, data loaded,\n",
      " 'Target': 23\n",
      " 'NonTarget': 114>\n",
      "  warn(f\"warnEpochs {epochs}\")\n",
      "c:\\Users\\doumif\\.conda\\envs\\py310\\lib\\site-packages\\moabb\\datasets\\preprocessing.py:278: UserWarning: warnEpochs <Epochs | 149 events (all good), 0 – 1 s (baseline off), ~74.6 MiB, data loaded,\n",
      " 'Target': 25\n",
      " 'NonTarget': 124>\n",
      "  warn(f\"warnEpochs {epochs}\")\n",
      "c:\\Users\\doumif\\.conda\\envs\\py310\\lib\\site-packages\\moabb\\datasets\\preprocessing.py:278: UserWarning: warnEpochs <Epochs | 124 events (all good), 0 – 1 s (baseline off), ~62.1 MiB, data loaded,\n",
      " 'Target': 21\n",
      " 'NonTarget': 103>\n",
      "  warn(f\"warnEpochs {epochs}\")\n",
      "c:\\Users\\doumif\\.conda\\envs\\py310\\lib\\site-packages\\moabb\\datasets\\preprocessing.py:278: UserWarning: warnEpochs <Epochs | 124 events (all good), 0 – 1 s (baseline off), ~62.1 MiB, data loaded,\n",
      " 'Target': 21\n",
      " 'NonTarget': 103>\n",
      "  warn(f\"warnEpochs {epochs}\")\n",
      "c:\\Users\\doumif\\.conda\\envs\\py310\\lib\\site-packages\\moabb\\datasets\\preprocessing.py:278: UserWarning: warnEpochs <Epochs | 149 events (all good), 0 – 1 s (baseline off), ~74.6 MiB, data loaded,\n",
      " 'Target': 24\n",
      " 'NonTarget': 125>\n",
      "  warn(f\"warnEpochs {epochs}\")\n",
      "c:\\Users\\doumif\\.conda\\envs\\py310\\lib\\site-packages\\moabb\\datasets\\preprocessing.py:278: UserWarning: warnEpochs <Epochs | 142 events (all good), 0 – 1 s (baseline off), ~71.1 MiB, data loaded,\n",
      " 'Target': 24\n",
      " 'NonTarget': 118>\n",
      "  warn(f\"warnEpochs {epochs}\")\n",
      "c:\\Users\\doumif\\.conda\\envs\\py310\\lib\\site-packages\\moabb\\datasets\\preprocessing.py:278: UserWarning: warnEpochs <Epochs | 124 events (all good), 0 – 1 s (baseline off), ~62.1 MiB, data loaded,\n",
      " 'Target': 21\n",
      " 'NonTarget': 103>\n",
      "  warn(f\"warnEpochs {epochs}\")\n",
      "c:\\Users\\doumif\\.conda\\envs\\py310\\lib\\site-packages\\moabb\\datasets\\preprocessing.py:278: UserWarning: warnEpochs <Epochs | 124 events (all good), 0 – 1 s (baseline off), ~62.1 MiB, data loaded,\n",
      " 'Target': 20\n",
      " 'NonTarget': 104>\n",
      "  warn(f\"warnEpochs {epochs}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding metadata with 3 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\doumif\\.conda\\envs\\py310\\lib\\site-packages\\moabb\\datasets\\preprocessing.py:278: UserWarning: warnEpochs <Epochs | 142 events (all good), 0 – 1 s (baseline off), ~71.1 MiB, data loaded,\n",
      " 'Target': 24\n",
      " 'NonTarget': 118>\n",
      "  warn(f\"warnEpochs {epochs}\")\n",
      "c:\\Users\\doumif\\.conda\\envs\\py310\\lib\\site-packages\\moabb\\datasets\\preprocessing.py:278: UserWarning: warnEpochs <Epochs | 137 events (all good), 0 – 1 s (baseline off), ~68.6 MiB, data loaded,\n",
      " 'Target': 23\n",
      " 'NonTarget': 114>\n",
      "  warn(f\"warnEpochs {epochs}\")\n",
      "c:\\Users\\doumif\\.conda\\envs\\py310\\lib\\site-packages\\moabb\\datasets\\preprocessing.py:278: UserWarning: warnEpochs <Epochs | 149 events (all good), 0 – 1 s (baseline off), ~74.6 MiB, data loaded,\n",
      " 'Target': 24\n",
      " 'NonTarget': 125>\n",
      "  warn(f\"warnEpochs {epochs}\")\n",
      "c:\\Users\\doumif\\.conda\\envs\\py310\\lib\\site-packages\\moabb\\datasets\\preprocessing.py:278: UserWarning: warnEpochs <Epochs | 137 events (all good), 0 – 1 s (baseline off), ~68.6 MiB, data loaded,\n",
      " 'Target': 22\n",
      " 'NonTarget': 115>\n",
      "  warn(f\"warnEpochs {epochs}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\doumif\\.conda\\envs\\py310\\lib\\site-packages\\moabb\\paradigms\\base.py:350: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  X = mne.concatenate_epochs(X)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding metadata with 3 columns\n",
      "3304 matching events found\n",
      "No baseline correction applied\n"
     ]
    }
   ],
   "source": [
    "# Verify the epochs shape, labels, and metadata of EPFLP300-4 if wanted\n",
    "# Epoch shape >>> trials X channels X time_samples\n",
    "\n",
    "paradigm = P300()\n",
    "dataset = moabb.datasets.EPFLP300()\n",
    "subjects = dataset.subject_list\n",
    "\n",
    "# set return epochs to False to see raw data \n",
    "epoch, label, metas = paradigm.get_data(dataset=dataset, subjects = [subjects[0]], return_epochs = True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_labels = dataset.event_id\n",
    "event_labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch.ch_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\doumif\\\\work\\\\OfficeWork\\\\BCI Databases\\\\CSV\\\\P300\\\\EPFLP300-4\\\\subject_01_session_01.csv',\n",
       " 'C:\\\\Users\\\\doumif\\\\work\\\\OfficeWork\\\\BCI Databases\\\\CSV\\\\P300\\\\EPFLP300-4\\\\subject_01_session_02.csv',\n",
       " 'C:\\\\Users\\\\doumif\\\\work\\\\OfficeWork\\\\BCI Databases\\\\CSV\\\\P300\\\\EPFLP300-4\\\\subject_01_session_03.csv',\n",
       " 'C:\\\\Users\\\\doumif\\\\work\\\\OfficeWork\\\\BCI Databases\\\\CSV\\\\P300\\\\EPFLP300-4\\\\subject_01_session_04.csv',\n",
       " 'C:\\\\Users\\\\doumif\\\\work\\\\OfficeWork\\\\BCI Databases\\\\CSV\\\\P300\\\\EPFLP300-4\\\\subject_02_session_01.csv',\n",
       " 'C:\\\\Users\\\\doumif\\\\work\\\\OfficeWork\\\\BCI Databases\\\\CSV\\\\P300\\\\EPFLP300-4\\\\subject_02_session_02.csv',\n",
       " 'C:\\\\Users\\\\doumif\\\\work\\\\OfficeWork\\\\BCI Databases\\\\CSV\\\\P300\\\\EPFLP300-4\\\\subject_02_session_03.csv',\n",
       " 'C:\\\\Users\\\\doumif\\\\work\\\\OfficeWork\\\\BCI Databases\\\\CSV\\\\P300\\\\EPFLP300-4\\\\subject_02_session_04.csv',\n",
       " 'C:\\\\Users\\\\doumif\\\\work\\\\OfficeWork\\\\BCI Databases\\\\CSV\\\\P300\\\\EPFLP300-4\\\\subject_03_session_01.csv',\n",
       " 'C:\\\\Users\\\\doumif\\\\work\\\\OfficeWork\\\\BCI Databases\\\\CSV\\\\P300\\\\EPFLP300-4\\\\subject_03_session_02.csv',\n",
       " 'C:\\\\Users\\\\doumif\\\\work\\\\OfficeWork\\\\BCI Databases\\\\CSV\\\\P300\\\\EPFLP300-4\\\\subject_03_session_03.csv',\n",
       " 'C:\\\\Users\\\\doumif\\\\work\\\\OfficeWork\\\\BCI Databases\\\\CSV\\\\P300\\\\EPFLP300-4\\\\subject_03_session_04.csv',\n",
       " 'C:\\\\Users\\\\doumif\\\\work\\\\OfficeWork\\\\BCI Databases\\\\CSV\\\\P300\\\\EPFLP300-4\\\\subject_04_session_01.csv',\n",
       " 'C:\\\\Users\\\\doumif\\\\work\\\\OfficeWork\\\\BCI Databases\\\\CSV\\\\P300\\\\EPFLP300-4\\\\subject_04_session_02.csv',\n",
       " 'C:\\\\Users\\\\doumif\\\\work\\\\OfficeWork\\\\BCI Databases\\\\CSV\\\\P300\\\\EPFLP300-4\\\\subject_04_session_03.csv',\n",
       " 'C:\\\\Users\\\\doumif\\\\work\\\\OfficeWork\\\\BCI Databases\\\\CSV\\\\P300\\\\EPFLP300-4\\\\subject_04_session_04.csv',\n",
       " 'C:\\\\Users\\\\doumif\\\\work\\\\OfficeWork\\\\BCI Databases\\\\CSV\\\\P300\\\\EPFLP300-4\\\\subject_06_session_01.csv',\n",
       " 'C:\\\\Users\\\\doumif\\\\work\\\\OfficeWork\\\\BCI Databases\\\\CSV\\\\P300\\\\EPFLP300-4\\\\subject_06_session_02.csv',\n",
       " 'C:\\\\Users\\\\doumif\\\\work\\\\OfficeWork\\\\BCI Databases\\\\CSV\\\\P300\\\\EPFLP300-4\\\\subject_06_session_03.csv',\n",
       " 'C:\\\\Users\\\\doumif\\\\work\\\\OfficeWork\\\\BCI Databases\\\\CSV\\\\P300\\\\EPFLP300-4\\\\subject_06_session_04.csv',\n",
       " 'C:\\\\Users\\\\doumif\\\\work\\\\OfficeWork\\\\BCI Databases\\\\CSV\\\\P300\\\\EPFLP300-4\\\\subject_07_session_01.csv',\n",
       " 'C:\\\\Users\\\\doumif\\\\work\\\\OfficeWork\\\\BCI Databases\\\\CSV\\\\P300\\\\EPFLP300-4\\\\subject_07_session_02.csv',\n",
       " 'C:\\\\Users\\\\doumif\\\\work\\\\OfficeWork\\\\BCI Databases\\\\CSV\\\\P300\\\\EPFLP300-4\\\\subject_07_session_03.csv',\n",
       " 'C:\\\\Users\\\\doumif\\\\work\\\\OfficeWork\\\\BCI Databases\\\\CSV\\\\P300\\\\EPFLP300-4\\\\subject_07_session_04.csv',\n",
       " 'C:\\\\Users\\\\doumif\\\\work\\\\OfficeWork\\\\BCI Databases\\\\CSV\\\\P300\\\\EPFLP300-4\\\\subject_08_session_01.csv',\n",
       " 'C:\\\\Users\\\\doumif\\\\work\\\\OfficeWork\\\\BCI Databases\\\\CSV\\\\P300\\\\EPFLP300-4\\\\subject_08_session_02.csv',\n",
       " 'C:\\\\Users\\\\doumif\\\\work\\\\OfficeWork\\\\BCI Databases\\\\CSV\\\\P300\\\\EPFLP300-4\\\\subject_08_session_03.csv',\n",
       " 'C:\\\\Users\\\\doumif\\\\work\\\\OfficeWork\\\\BCI Databases\\\\CSV\\\\P300\\\\EPFLP300-4\\\\subject_08_session_04.csv',\n",
       " 'C:\\\\Users\\\\doumif\\\\work\\\\OfficeWork\\\\BCI Databases\\\\CSV\\\\P300\\\\EPFLP300-4\\\\subject_09_session_01.csv',\n",
       " 'C:\\\\Users\\\\doumif\\\\work\\\\OfficeWork\\\\BCI Databases\\\\CSV\\\\P300\\\\EPFLP300-4\\\\subject_09_session_02.csv',\n",
       " 'C:\\\\Users\\\\doumif\\\\work\\\\OfficeWork\\\\BCI Databases\\\\CSV\\\\P300\\\\EPFLP300-4\\\\subject_09_session_03.csv',\n",
       " 'C:\\\\Users\\\\doumif\\\\work\\\\OfficeWork\\\\BCI Databases\\\\CSV\\\\P300\\\\EPFLP300-4\\\\subject_09_session_04.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Forward your file with all .csv of the dataset\n",
    "\n",
    "file_dir = \"C:\\\\Users\\\\doumif\\\\work\\\\OfficeWork\\\\BCI Databases\\\\CSV\\\\P300\\\\EPFLP300-4\"\n",
    "all_files = [os.path.join(file_dir, file) for file in os.listdir(file_dir)]\n",
    "all_files #check if you have .csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calc windowlength\n",
    "samplingrate = 256 # original data was downsampled to 256Hz, original = 2048Hz(available in dataset description)\n",
    "windowlength = np.diff(dataset.interval)[0]*samplingrate\n",
    "windowlength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YML creator \n",
    "#The Meta data of the Data set\n",
    "#These variables and parameters should address once for each data set.  \n",
    "\n",
    "#######################################Acquisition#######################################\n",
    "filter= 'Low-Pass 83Hz (Butterworth order 4 zero phase) for downsampling'\n",
    "ground='None'\n",
    "hardware= 'Biosemi Active Two amplifier'\n",
    "reference= 'MA1/MA2'\n",
    "samplingrate = 256 # original data was downsampled to 256Hz, original = 2048Hz(available in dataset description)\n",
    "sensors = epoch.ch_names\n",
    "sensortype ='Ag/AgCl Wet electrodes'\n",
    "software= 'Dynamic Link Library (DLL) with MEX interface (matlab)'\n",
    "\n",
    "######################################Documentation######################################\n",
    "description= 'https://documents.epfl.ch/groups/m/mm/mmspg/www/BCI/p300/readme.pdf'\n",
    "doi= 'https://doi.org/10.1016/j.jneumeth.2007.03.005'\n",
    "investigators = \"Ulrich Hoffmann, Jean-Marc Vesin, Karin Diserens, Touradj Ebrahimi\"\n",
    "place = \"Ecole Polytechnique Federale de Lausanne (EPFL), Switzerland\"\n",
    "repository= 'https://www.epfl.ch/labs/mmspg/research/page-58317-en-html/bci-2/bci_datasets/'\n",
    "##id\n",
    "condition= 'Run 4 - Door' \n",
    "database= 'EPFLP300'  \n",
    "paradigm= 'P300'\n",
    "timestamp= 2007\n",
    "\n",
    "##########################################Stim###########################################\n",
    "####Labels\n",
    "event_labels = dict(NonTarget=1, Target=2)\n",
    "nclasses= len(event_labels.keys())\n",
    "offset= 0\n",
    "windowlength = 256 \n",
    "\n",
    "#########################################Subjects########################################  \n",
    "subject_ids = dataset.subject_list \n",
    "subjects=len(subject_ids)\n",
    "sessions = 4\n",
    "runs= 1\n",
    "\n",
    "#function of YMLcreator\n",
    "def YMLcreator():\n",
    "\n",
    "\n",
    "  d = dict(acquisition=dict\n",
    "          (filter= str(filter),\n",
    "          ground=str(ground),\n",
    "          hardware=str(hardware),\n",
    "          reference=str(reference),\n",
    "          samplingrate= samplingrate,\n",
    "          sensors= sensors,\n",
    "          sensortype=str(sensortype),\n",
    "          software=str(software)\n",
    "          )\n",
    "          ,documentation=dict\n",
    "          (description=str(description),\n",
    "          doi=str(doi),\n",
    "          investigators=str(investigators),\n",
    "          place=str(place),\n",
    "          repository=str(repository)\n",
    "          )              \n",
    "          ,\n",
    "          formatversion=str('0.0.1')\n",
    "          ,\n",
    "          id=dict\n",
    "          (condition=str(condition),\n",
    "          database=str(database),\n",
    "          paradigm=str(paradigm),\n",
    "          subject=subject,\n",
    "          session=session,\n",
    "          run=4,\n",
    "          timestamp=timestamp,\n",
    "          )\n",
    "          ,stim=dict\n",
    "          (labels=event_labels,\n",
    "          nclasses=nclasses,\n",
    "          offset=offset,\n",
    "          windowlength=windowlength,\n",
    "          )\n",
    "          \n",
    "  )\n",
    "  return d  \n",
    "\n",
    "i=-1 #don't change i\n",
    "\n",
    "for s_index, subject in enumerate(subject_ids):\n",
    "    for session in range(1, sessions + 1):\n",
    "        i = i + 1\n",
    "        filepath = all_files[i]\n",
    "        for run in range(1, runs + 1):\n",
    "            d = YMLcreator()\n",
    "            newpath = os.path.splitext(filepath)[0] + '.yml'\n",
    "            with open(newpath, 'w') as file:\n",
    "              documents = yaml.dump(d, file)\n",
    "\n",
    "              file.write(\"\"\"\n",
    "##############################################################################\n",
    "#                     GIPSA-lab standard for EEG time series (version 0.0.1) #\n",
    "#                                Authors : Pedro Rodrigues and Marco Congedo #\n",
    "#                                                        November 15th, 2019 #\n",
    "##############################################################################\n",
    "\n",
    "# This format has been conceived for easily sharing EEG data in Python and \n",
    "# Julia. Each file is understood as a separate recording. Data consist of two\n",
    "# files. They have the same name and extensions `npz` and `yml` (this file).\n",
    "\n",
    "# The `npz` file typically holds the EEG data matrix, a real matrix of \n",
    "# dimension num. of samples x num. of electrodes and a vector of integer with\n",
    "# the tags for stimulations, with as many entries as number of samples. The \n",
    "# tags are 0 (zero) for no stimulation and then employs the natural numbers \n",
    "# (1, 2,...) for different stimulation classes.\n",
    "\n",
    "# The `yml` file holds all meta-data info of the recording in `yml` format. \n",
    "# It holds two fields and four dictionaries:\n",
    "\n",
    "# FIELDS:\n",
    "#\n",
    "# - paradigm: (string) the experimental paradigm, e.g., P300, MI, ... \n",
    "#             for Brain Computer Interfaces experiments\n",
    "#\n",
    "# - formatversion: (version) version of this metadata specification\n",
    "\n",
    "# DICTIONARIES:\n",
    "#\n",
    "# - acquisition: (dictionary)\n",
    "#\n",
    "#   - filter: (string) filter setting of the EEG acquisition machine, \n",
    "#             specifying the type and specification. Ex: \"Band-pass digital \n",
    "#             filter (0.01-70Hz)\"\n",
    "#   - ground: (string) location of the sensor used as ground. Ex: \"Fpz\"\n",
    "#   - reference: (string) location of the sensor used as reference for the \n",
    "#                recording. Ex: \"A1\"\n",
    "#   - hardware: (string) the commercial name and producer of the the EEG \n",
    "#               acquisition machine. Ex: \"actiCHamp, Brain Products GmbH \n",
    "#               (Germany), DC amplifiers\"\n",
    "#   - software: (string) software used for acquiring and storing the data. \n",
    "#               Ex: OpenViBE, INRIA (France)\n",
    "#   - samplingrate:(int) sampling rate. Ex: 128\n",
    "#   - sensors: (array-like of strings) location of the sensors, excluding \n",
    "#              ground and reference.\n",
    "#   - sensortype: (string) type, material and product name of electrodes. \n",
    "#                 Ex: Ag/AgCl, Braincap, Brain Products GmbH (Germany)\n",
    "#\n",
    "# - documentation: (dictionary)\n",
    "#\n",
    "#   - description: (string) link to a file or website describing the dataset\n",
    "#   - doi: (string) digital object identifier of the dataset's documentation\n",
    "#   - repository:  (string) link to the online repository where the data can \n",
    "#                  be downloaded\n",
    "#\n",
    "# - id: (dictionary)\n",
    "#   \n",
    "#   - database: (string) name of the database\n",
    "#\n",
    "# - stim: (dictionary)\n",
    "#\n",
    "#   - labels: (dictionary) dictionary with the labels and code of the \n",
    "#             stimulations\n",
    "#       - nclasses: (int) number of classes for the stimulations\n",
    "#       - offset: (int) offset, given in number of samples, with respect to \n",
    "#                 stimulation samples, defining the beginning of trials\n",
    "#       - windowlength: (int) size of the window, given in number of sample, \n",
    "#                       defining the duration of trials\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##NPZ creator\n",
    "\n",
    "#the Structure of the Data sets \n",
    "\"\"\"\n",
    "The data should be in the form of  Samples*Channels, where channels may include a column for the time,\n",
    "a column for the ground, a colum for the trigger, and a colum for the target. So, you need to drop the\n",
    "columns of time and ground and add the two columns of trigger and target together(If you have not already done so).\n",
    "Sometime, databases might come with reference electrodes(corresponding to 1 or 2 coloumns), refer to the actual data description by authors if they\n",
    "used it for their analysis or only for reference. \n",
    "\"\"\"\n",
    "\n",
    "timestamp_col = 0  # timestamp col\n",
    "target_col= 35 # Stim target\n",
    "reref_col1 = 33 # re referencing col 1 : MA1\n",
    "reref_col2 = 34 # re referencing col 2 : MA2\n",
    "\n",
    "#NPZ creator\n",
    "\n",
    "def csv2npz(filepath):\n",
    "\n",
    "    df= pd.read_csv(filepath, header=0) # header = 0 to remove header\n",
    "    df= np.array(df)\n",
    "    \n",
    "    #DATA is in the shape of Samples*Channels \n",
    "    DATA=np.float64(df[:,0:35])\n",
    "\n",
    "    # Re referencing\n",
    "    reref_signal1 = DATA[:, reref_col1]\n",
    "    reref_signal2 = DATA[:, reref_col2]\n",
    "    reference_signal = (reref_signal1 + reref_signal2) / 2.0\n",
    "\n",
    "    DATA = DATA - reference_signal.reshape(-1, 1)\n",
    "\n",
    "    #remove the time column, reref cols\n",
    "    DATA = np.delete(DATA, [timestamp_col, reref_col1, reref_col2], axis=1)\n",
    "    DATA = DATA * 1e6 # from V to µV\n",
    "    DATA = np.float32(DATA)\n",
    "\n",
    "    print(DATA.shape)\n",
    "    \n",
    "    #STIM is the sum of the two trigger and target columns \n",
    "    STIM = np.int16(df[:,target_col])\n",
    "    \n",
    "    # use it to count NT/T ratio\n",
    "    unique_values, counts = np.unique(STIM, return_counts=True)\n",
    "    for value, count in zip(unique_values, counts):\n",
    "        print(f\"In {filepath} : Number of {value} = {count}\")\n",
    "    \n",
    "    newpath=os.path.splitext(filepath)[0] + '.npz'\n",
    "    np.savez(newpath,data=DATA , stim=STIM)\n",
    "\n",
    "for f in all_files:\n",
    "    csv2npz(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
